# 测试策略

## 1. 概述

本文档描述了中文学习平台的测试策略，包括单元测试、集成测试、端到端测试、性能测试、错误处理测试和安全性测试。

## 2. 测试类型

### 2.1 单元测试

#### 目的

- 验证每个模块（例如模型、服务、工具函数）的功能是否正确。
- 确保每个模块的逻辑正确性，覆盖正常情况、边界情况和异常情况。
- 提供快速反馈，方便开发人员及时发现和修复问题。

#### 方法

- 使用 `pytest` 测试框架来组织和运行测试。
- 为每个模块编写独立的测试用例，确保测试用例的独立性。
- 使用 `fixture` 或其他方法，在测试前清理测试数据。
- 使用 `assert` 语句来验证 API 的返回结果是否符合预期。
- 覆盖正常情况、边界情况和异常情况，例如：
  - 有效输入：验证 API 在有效输入下的正确性。
  - 无效输入：验证 API 在无效输入下的错误处理逻辑。
  - 边界值：验证 API 在边界值附近的表现。
  - 缺失参数：验证 API 在缺少必填参数时的错误处理。
  - 错误码：确保返回的错误码与 API 文档中的定义一致。
  - 数据类型：验证 API 对不同数据类型的处理是否正确。
  - 分页：验证 API 分页逻辑是否正确。

#### 测试范围

- **模型层**: 测试数据模型的定义和操作，例如数据类型的验证，数据的 CRUD 操作。
- **服务层**: 测试业务逻辑的处理，例如故事的生成、词语的查询、场景的管理，以及**生词率**计算逻辑的正确性。 **详细定义和计算方法见 [`生词与生词率.md`](生词与生词率.md)**。
  - **自定义分词**: 确保自定义分词的逻辑正确，并且能够正确处理各种情况，例如：
    - 文本中包含词汇表中的词语。
    - 文本中不包含词汇表中的词语。
    - 文本中包含多个词汇表中的词语。
    - 验证 `_load_known_words` 函数是否正确加载了已知词汇和词性， **并且不包含目标级别的词语**。
    - 验证 `calculate_vocabulary_rate` 函数是否正确计算了**生词率**。
  - **多轮对话**: 验证多轮对话的逻辑， 以及提示语的生成， 测试状态机是否能够正确管理对话流程， 以及如何根据用户反馈动态调整对话策略。
- **工具函数**: 测试工具函数的正确性，例如错误处理函数、API 认证函数、以及其他工具函数。
- **提示语生成**: 验证提示语生成函数的正确性，确保生成的提示语符合预期的格式和内容。

#### 最佳实践

- **提前规划与设计**: 在开始测试之前，务必充分理解产品需求文档 (PRD) 和 API 设计指南，明确每个 API 的功能、输入参数、输出格式和错误处理逻辑。
- **设计测试用例**: 根据需求设计全面的测试用例，覆盖正常情况、边界情况和异常情况。
- **自底向上测试**: 从最底层的模型层开始测试，逐步向上测试服务层和 API 路由层。
- **重视错误信息与调试**: 确保 API 返回的错误信息清晰、具体，方便定位问题。可以使用自定义的错误码和错误信息，提供更详细的错误上下文。
- **日志记录**: 在代码中添加日志记录，方便追踪程序运行过程中的问题。
- **测试代码的独立性**: 每个测试用例都应该独立运行，不依赖于其他测试用例产生的数据。可以使用 fixture 或其他方法，在测试前清理测试数据。
- **断言的准确性**: 使用断言来验证 API 的返回结果是否符合预期。确保断言的条件正确，能够准确地判断测试是否通过。
- **测试代码的可读性**: 编写可读性高的测试代码，方便其他人理解和维护。
- **避免重复代码**: 使用 fixture 或其他方法，避免测试代码中的重复代码。
- **持续集成与自动化测试**: 将测试代码自动化，每次修改代码后都自动运行测试，及时发现问题。
- **单元测试应该重点测试核心代码**: 例如**生词率**计算，多轮对话的逻辑，API 鉴权， 和 API 参数验证， 并且尽量提高代码覆盖率。
- **生成测试覆盖率报告**: 可以使用 `pytest-cov` 插件生成测试覆盖率报告。

  - 安装 `pytest-cov`:

    ```bash
    pip install pytest-cov
    ```

  - 运行测试并生成报告：

    ```bash
     pytest --cov=app --cov-report term-missing
    ```

    - `--cov=app`: 指定要测试的代码目录。
    - `--cov-report term-missing`: 在终端显示测试覆盖率报告，并显示未覆盖的代码行。

### 2.2 集成测试

#### 目的

- 验证 API 接口的请求参数验证、业务逻辑处理和响应格式是否正确。
- 确保 API 能够正确地与其他模块进行交互。

#### 方法

- 使用测试客户端 (`Flask` 提供的 `test_client`) 来模拟 API 请求。
- 验证 API 的返回码、响应信息和数据是否符合预期。

#### 测试范围

- **API 接口**: 测试 API 接口的请求参数验证、业务逻辑处理和响应格式是否正确，包括：
  - 故事生成 API： 验证请求参数，**生词率**计算，重点词汇，和 JSON 响应, 包括 `story_word_count` 参数。
  - 场景管理 API：验证场景的 CRUD 操作。
  - 词语查询 API：验证分页查询和参数过滤。
  - 故事升级/降级 API： 验证故事的升级和降级逻辑。
  - **多轮对话**: 验证多轮对话的逻辑和提示语的生成。

### 2.3 端到端测试

#### 目的

- 模拟用户完整的操作流程，验证多个模块的协同工作是否正确。
- 验证数据流转和整体功能是否正常。

#### 方法

- 模拟用户完整的操作流程，例如：
  1.  用户通过 API 创建一个场景。
  2.  用户通过 API 添加一些词语。
  3.  用户通过 API 生成一个故事。
  4.  用户通过 API 查询生成的故事。
  5.  用户通过 API 升级/降级故事。
- 验证整个流程是否正确，包括数据流转、业务逻辑和用户界面（如果存在）。

### 2.4 性能测试

#### 目的

- 验证 API 的响应时间、吞吐量和稳定性是否满足要求。
- 分析性能瓶颈，并进行相应的优化。

#### 方法

- 使用性能测试工具，对 API 进行压力测试。
- 监控 API 的响应时间、吞吐量、CPU 使用率、内存使用率等指标。
- 根据测试结果，优化代码或服务器配置。

#### 测试指标

- **API 的响应时间**:
  - 故事生成 API 的响应时间应控制在 2 秒以内。
  - 场景管理、词语查询 API 的响应时间应控制在 500 毫秒以内。
  - 故事升级/降级 API 的响应时间应控制在 1 秒以内。
- **吞吐量**:
  - API 的吞吐量应该能够满足用户的需求，需要进行压力测试，模拟多用户并发访问。
- **多轮对话性能**:
  - 验证多轮对话的响应时间。

### 2.5 错误处理测试

#### 目的

- 验证 API 对各种错误情况的处理是否正确。
- 确保 API 返回的错误码和错误信息清晰、准确。

#### 方法

- 模拟各种错误情况，例如：
  - 参数错误：缺少必填字段、字段类型错误、字段值超出范围。
  - 资源不存在：场景未找到、词语未找到、故事未找到。
  - 未授权：API Key 缺失、API Key 无效